import boto3
import copy
import json
import requests
import time
from builtins import range
from threading import Thread
from queue import Queue
import re
import uuid

from splunk import AuthorizationFailed
import splunk.pdf.availability as pdf_availability
import splunk.pdf.pdfgen_utils as pdfgen_utils
import splunk.entity as entity
import splunk.search as search
import splunk.rest as rest
import splunk.safe_lxml_etree as et
from splunk.pdf.pdfgen_search import InlineSearchManager, SavedSearchManager, PostProcessSearchManager
import splunk.util

logger = pdfgen_utils.getLogger()
DEFAULT_TIMEOUT = 3600
READ_ONLY_TOKEN_NAMESPACES = {'env'}
STUDIO_ENGINE_TAG = 'studioemerald'
MAX_CONCURRENT_SEARCHES = 3


def parseDataSourceSearchResults(results, fieldOrder):
    """
    Parses ResultSet results and fieldOrder into fields and columns format. Do not include fields from fieldOrder
    if there are no columns. Modelled after splunk.pdfgen_table.addRowFromSearchResult()
    """
    parsedResults = {}
    for i, resultRow in enumerate(results):
        for field in resultRow.fields:
            existingFieldValues = parsedResults.get(field, [])
            newFieldValues = resultRow[field]
            # make new field values all strings and append to existingFieldValues to add to parsedResults
            if isinstance(newFieldValues, search.RawEvent):
                fieldValuesStr = newFieldValues
                existingFieldValues.append(str(fieldValuesStr))
            elif len(newFieldValues) > 1:
                fieldValueStrings = [str(x) for x in newFieldValues]
                existingFieldValues.append(fieldValueStrings)
            else:
                fieldValuesStr = newFieldValues[0]
                existingFieldValues.append(str(fieldValuesStr))
            parsedResults[field] = existingFieldValues
    columns = []
    fields = []
    keys = parsedResults.keys()
    for field in fieldOrder:
        if field in keys:
            columnValues = parsedResults[field]
            if len(columnValues) > 0:
                columns.append(columnValues)
                fields.append(field)
    return fields, columns


def getDataSourceSearchResults(searchManager, resolvedDataSources, deletedDataSources, ds, checkRiskyCommand):
    """
    Dispatches searchManager to retrieve search results for ds in fields/columns format. If the search completes
    successfully, the dataSource is added to resolvedDataSources with the corresponding results stored in a ds.test
    format. If there is an error, the dataSource is added to deletedDataSources with the corresponding error message.
    """
    logger.debug("getting search results for dataSource %s" % ds)
    if searchManager is None:
        errorMessage = "Error for DataSource %s: Unable to create search." % ds
        logger.error(errorMessage)
        deletedDataSources[ds] = errorMessage
        return

    timeoutTime = int(time.time()) + DEFAULT_TIMEOUT
    try:
        searchManager.dispatch(checkRiskyCommand=checkRiskyCommand)
        while not searchManager.isComplete() and not searchManager.isRealtime():
            time.sleep(1)
            currentTime = int(time.time())
            timedOut = currentTime > timeoutTime
            if timedOut:
                searchManager.cancel()
                errorMessage = "Error for DataSource %s: Search timed out after %s seconds." % (ds, DEFAULT_TIMEOUT)
                logger.error(errorMessage)
                deletedDataSources[ds] = errorMessage
                return

        results = searchManager.results()
        fieldOrder = results.fieldOrder
        fields, columns = parseDataSourceSearchResults(results, fieldOrder)

        # modify data obj to contain search results as ds.test
        dsTest = {
            'type': 'ds.test',
            'options': {
                'data': {
                    'fields': fields,
                    'columns': columns
                },
                'meta': {},
            }
        }
        resolvedDataSources[ds] = dsTest
    except Exception as e:
        if isinstance(e, AuthorizationFailed) and checkRiskyCommand and 'check_risky_command' in e.extendedMessages:
            # This is a special case for chain searches containing risky commands, which throw an AuthorizationFailed
            # exception which does not contain any useful info for security reasons. Because of this, we need to return
            # a more generic risky commands error message.
            errorMessage = "Error for DataSource %s: %s" % (ds, 'Found potentially risky commands.')
        else:
            errorMessage = "Error for DataSource %s: %s" % (ds, str(e))
        logger.error(errorMessage)
        deletedDataSources[ds] = errorMessage
        return


def getDataURI(url, collection, sessionKey):
    """
    If the given URL is a kvstore URL, return the data URI. If the given URL is None or if
    there is an error accessing the kvstore, return None. Otherwise return the original URL.
    """
    kvstoreIdentifier = 'splunk-enterprise-kvstore://'
    if url is None or not url.startswith(kvstoreIdentifier):
        return url
    try:
        key = url.split(kvstoreIdentifier)[1]
        uri = entity.buildEndpoint(
            ['storage', 'collections', 'data', collection, key],
            namespace='splunk-dashboard-studio'
        )
        jobResponseHeaders, jobResponseBody = rest.simpleRequest(uri, method='GET', sessionKey=sessionKey, getargs={'output_mode':'json'})
        searchJob = json.loads(jobResponseBody)
        return searchJob.get('dataURI')
    except Exception:
        return None


def getSearchManager(dataSourceName, dataSources, managerMap, namespace, owner, sessionKey):
    """
    Return the search manager for the corresponding dataSourceName if it already exists in the managerMap. Otherwise,
    create and return a search manager based on the type of dataSource, or None if there is an error.
    Add the search manager to the managerMap.
    """
    if dataSourceName in managerMap:
        return managerMap[dataSourceName]

    searchManager = None
    dataSource = dataSources.get(dataSourceName, {})
    options = dataSource.get('options', {})
    type = dataSource.get('type')

    if type == 'ds.search':
        query = options.get('query')
        if query is not None:
            earliest = options.get('queryParameters', {}).get('earliest')
            latest = options.get('queryParameters', {}).get('latest')
            searchManager = InlineSearchManager(query, earliest, latest, namespace, owner, sessionKey)
    elif type == 'ds.savedSearch':
        name = options.get('ref')
        if name is not None:
            searchManager = SavedSearchManager(name, namespace=namespace, owner=owner, sessionKey=sessionKey)
    elif type == 'ds.chain':
        query = options.get('query')
        parentDataSourceName = options.get('extend')
        if query is not None and parentDataSourceName is not None:
            parentSearchManager = getSearchManager(parentDataSourceName, dataSources, managerMap, namespace, owner, sessionKey)
            if parentSearchManager is not None:
                searchManager = PostProcessSearchManager(query, parentSearchManager, namespace, owner, sessionKey)

    if searchManager is None:
        logger.error('Unable to create search manager for dataSource %s.' % dataSourceName)

    managerMap[dataSourceName] = searchManager
    return searchManager


def getInputValues(definition):
    """
    Create a dictionary of input values and their default definitions. If no default definition exists, the input
    is not added. If the input is a time range, separate the earliest and latest values in the dictionary.
    """
    inputs = definition.get('inputs', {})
    inputValues = {}
    for input in inputs:
        inputType = inputs[input].get('type')
        defaultValue = inputs[input].get('options', {}).get('defaultValue')
        tokenName = inputs[input].get('options', {}).get('token')
        tokenNamespace = inputs[input].get('options', {}).get('tokenNamespace')
        # tokens in readonly namespaces shouldn't be modifiable by inputs
        if tokenNamespace not in READ_ONLY_TOKEN_NAMESPACES:
            if tokenName is not None and defaultValue is not None:
                if tokenNamespace is not None:
                    tokenName = '{}:{}'.format(tokenNamespace, tokenName)
                if inputType == 'input.timerange':
                    inputValues[tokenName + '.earliest'] = defaultValue.split(',')[0].strip()
                    inputValues[tokenName + '.latest'] = defaultValue.split(',')[1].strip()
                else:
                    inputValues[tokenName] = defaultValue
    return inputValues


def hasToken(string):
    """
    Return true if string contains any tokens.
    That is, any character(s) that start and end with a $ character
    """
    return re.search(r'(\$([\w:\.]+?)\$)', string)


def mergeOptions(options1, options2):
    """
    Return new object with merged options1 and options2, where options1 takes precedence
    """
    mergedOptions = copy.deepcopy(options2)
    mergedOptions.update(options1)
    return mergedOptions


def getMergedDefaultOptionsForDataSourceType(defaults, dataSourceType):
    """
    Return a merged set of global and type specific default options for the given data source type.
    Type specific default options take precedence over global default options. Exclude query from these options.
    """
    if defaults == {} or dataSourceType is None:
        return {}

    globalOptions = defaults.get('dataSources', {}).get('global', {}).get('options', {})
    typeSpecificOptions = defaults.get('dataSources', {}).get(dataSourceType, {}).get('options', {})
    mergedOptions = mergeOptions(typeSpecificOptions, globalOptions)
    if mergedOptions.get("query") is not None:
        del mergedOptions["query"]
    return mergedOptions


def getMergedOptionsForDataSource(dataSource, mergedDefaultOptions, defaults, dsName):
    """
    Return a merged set of default and data source specific options for the given data source.
    Data source specific options take precedence over default options.
    """
    logger.debug("getting merged options for dataSource %s" % dsName)
    dataSourceType = dataSource.get('type')
    dataSourceOptions = dataSource.get('options', {})
    if not mergedDefaultOptions.get(dataSourceType, {}):
        mergedDefaultOptions[dataSourceType] = getMergedDefaultOptionsForDataSourceType(defaults, dataSourceType)
    defaultOptions = mergedDefaultOptions[dataSourceType]
    return mergeOptions(dataSourceOptions, defaultOptions)

def runSearches(searchQueue, resolvedDataSources, deletedDataSources, checkRiskyCommand):
    """
    Runs all the searches in the searchQueue
    """
    while not searchQueue.empty():
        searchManager, datasourceId = searchQueue.get()
        getDataSourceSearchResults(searchManager, resolvedDataSources, deletedDataSources, datasourceId, checkRiskyCommand)
        searchQueue.task_done()


def modifyDataSourceSearches(definition, namespace, owner, sessionKey):
    """
    For every dataSource in the definition, run the search and replace it with the results as ds.test.
    The original definition is modified. If a dataSource search results in an error, the dataSource name 
    is added to the deletedDataSources dictionary, with the corresponding error message.
    """
    logger.debug("modifying dataSource searches")
    dataSources = definition.get('dataSources', {})
    dataSourcesCopy = copy.deepcopy(dataSources)
    defaults = definition.get('defaults', {})
    mergedDefaultOptions = {}
    managerMap = {}
    deletedDataSources = {}
    resolvedDataSources = {}
    searchQueue = Queue()
    checkRiskyCommand = pdfgen_utils.isRiskyCommandCheckDashboardEnabled(sessionKey)

    for ds in dataSourcesCopy:
        dataSource = dataSources[ds]
        dataSource['options'] = getMergedOptionsForDataSource(dataSource, mergedDefaultOptions, defaults, ds)
        if hasToken(json.dumps(dataSource)):
            errorMessage = "Error for DataSource %s: Specify default value for token." % ds
            logger.error(errorMessage)
            deletedDataSources[ds] = errorMessage
        elif dataSource.get('type') != 'ds.test':
            searchManager = getSearchManager(ds, dataSources, managerMap, namespace, owner, sessionKey)
            searchQueue.put((searchManager, ds))
    for _ in range(MAX_CONCURRENT_SEARCHES):
        thread = Thread(target=runSearches, args=[searchQueue, resolvedDataSources, deletedDataSources, checkRiskyCommand])
        thread.start()
    searchQueue.join()
    for ds in resolvedDataSources:
        logger.debug('Resolving datasource %s' % ds)
        dataSources[ds] = resolvedDataSources[ds]
    for ds in deletedDataSources:
        logger.debug('Deleting datasource %s' % ds)
        del dataSources[ds]
    logger.debug(definition)
    return deletedDataSources


def createVizError(errorMessage = None):
    """
    Return viz.error with given errorMessage
    """
    return {
        'type': 'viz.error',
        'options': {
            'error': errorMessage
        },
        'dataSources': {},
        'eventHandlers': []
    } if errorMessage else {
        'type': 'viz.error',
        'options': {},
        'dataSources': {},
        'eventHandlers': []
    }


def modifyVizWithDeletedDataSources(definition, vizName, deletedDataSources):
    """
    If the given viz contains a dataSource that has been deleted, convert the viz to viz.error
    """
    logger.debug("checking for deleted dataSources in %s" % vizName)
    if deletedDataSources:
        for dataSource in definition['visualizations'][vizName].get('dataSources', {}):
            dataSourceName = definition['visualizations'][vizName]['dataSources'][dataSource]
            if dataSourceName in deletedDataSources.keys():
                definition['visualizations'][vizName] = createVizError(deletedDataSources[dataSourceName])
                break


def modifyBackgroundImageDataURI(definition, sessionKey):
    """
    If there is a backgroundImage, replace the URL with the data URI. If there
    is an error retrieving the data URI, delete the backgroundImage from layout options.
    """
    logger.debug('replacing background image URL with data URI')
    if definition.get('layout', {}).get('options', {}).get('backgroundImage'):
        url = definition['layout']['options']['backgroundImage'].get('src')
        dataURI = getDataURI(url, 'splunk-dashboard-images', sessionKey)
        if dataURI:
            definition['layout']['options']['backgroundImage']['src'] = dataURI
        else:
            del definition['layout']['options']['backgroundImage']


def modifyVizDataURI(definition, vizName, sessionKey):
    """
    For image or singlevalueicon visualizations, replace the URL with the data URI.
    If there is an error retrieving the data URI, convert the visualization to viz.error.
    """
    logger.debug("replacing image/icon URL with data URI for visualization %s" % vizName)
    viz = definition['visualizations'][vizName]
    vizType = viz.get('type')
    if vizType == 'viz.img' or vizType == 'splunk.image':
        url = viz.get('options', {}).get('src')
        dataURI = getDataURI(url, 'splunk-dashboard-images', sessionKey)
        if dataURI:
            definition['visualizations'][vizName]['options']['src'] = dataURI
        else:
            definition['visualizations'][vizName] = createVizError('Unable to display image')

    elif vizType == 'viz.singlevalueicon' or vizType == 'splunk.singlevalueicon':
        url = viz.get('options', {}).get('icon')
        dataURI = getDataURI(url, 'splunk-dashboard-icons', sessionKey)
        if dataURI:
            definition['visualizations'][vizName]['options']['icon'] = dataURI
        else:
            definition['visualizations'][vizName] = createVizError('Unable to display icon')


def uploadDefinitionToS3(definition, s3BucketName, cloudStack):
    """
    Uploads the dashboard definition to the given s3 bucket and returns a presigned GET URL for it.
    """
    objectId = uuid.uuid4().hex
    
    client = boto3.client('s3')
    client.put_object(
        Body=json.dumps(definition),
        Bucket=s3BucketName,
        Key='dce/{}/definitions/{}.json'.format(cloudStack, objectId)
    )
    presignedUrl = client.generate_presigned_url('get_object',
        Params={
            'Bucket': s3BucketName,
            'Key': 'dce/{}/definitions/{}.json'.format(cloudStack, objectId)
        },
        ExpiresIn=1800  # 30 minutes
    )
    return presignedUrl

def getServerInfo(sessionKey):
    return entity.getEntity('/server', 'info', sessionKey=sessionKey)

def getUserInfo(sessionKey):
    return entity.getEntity('/authentication/', 'current-context', sessionKey=sessionKey)

def getEnvironmentTokens(sessionKey, namespace, dashboardName):
    """
    Gets standard environment tokens as mentioned here excluding locale
    https://docs.splunk.com/Documentation/Splunk/8.2.1/Viz/tokens#Use_global_tokens_to_access_environment_information
    """
    environment_tokens = {'env:app': namespace, 'env:page': dashboardName}
    try:
        serverInfo = getServerInfo(sessionKey)
        product_type = serverInfo.get('product_type')
        environment_tokens['env:product'] = product_type
        environment_tokens['env:version'] = serverInfo.get('version')

        is_product_types = ['enterprise', 'hunk', 'lite', 'lite_free']
        if product_type in is_product_types:
            environment_tokens['env:is_{}'.format(product_type)] = 'true'

        instanceType = serverInfo.get('instance_type')
        if instanceType:
            environment_tokens['env:instance_type'] = instanceType
            if instanceType == 'cloud':
                environment_tokens['env:is_cloud'] = 'true'

        if splunk.util.normalizeBoolean(serverInfo.get('isFree')):
            environment_tokens['env:is_free'] = 'true'

        user = getUserInfo(sessionKey)
        environment_tokens['env:user'] = user.get('username')
        environment_tokens['env:user_realname'] = user.get('realname')
        environment_tokens['env:user_email'] = user.get('email')

    except Exception as e:
        logger.error('Exception while fetching environment tokens: %s', str(e))

    return environment_tokens

def replaceTokens(definition, sessionKey, namespace, dashboardName):
    """
    Replace tokens in dataSources, defaults, and visualizations with the default values of tokens
    defined in the definition. Modifies the original definition.
    Note that in the following example, a token with an int type will remain an int
        definition = { "count": "$token$" }
    Whereas in the following example, a token with an int type will be converted to a string
        definition = { "query": "index = $token$" }
    """
    tokenValues = {}
    tokenValues.update(getInputValues(definition))
    tokenValues.update(getEnvironmentTokens(sessionKey, namespace, dashboardName))
    
    def _replaceTokensInValue(value):
        tokensInValue = re.findall(r'\$([^$]+?)\$', value)
        for token in tokensInValue:
            tokenString = '$' + token + '$'
            tokenValue = tokenValues.get(token)
            if tokenValue is None:
                continue
            if type(tokenValue) is int:
                if value == tokenString:
                    return tokenValue
                tokenValue = str(tokenValue)
            value = value.replace(tokenString, tokenValue)
        return value

    maxRecursionLevel = 10 # Same limit as in UDF
    def _replaceTokens(definition, recursionLevel=0):
        if isinstance(definition, str):
            definition = _replaceTokensInValue(definition)
        elif isinstance(definition, list) and recursionLevel <= maxRecursionLevel:
            for i, value in enumerate(definition):
                definition[i] = _replaceTokens(value, recursionLevel + 1)
        elif isinstance(definition, dict) and recursionLevel <= maxRecursionLevel:
            for key, value in definition.items():
                definition[key] = _replaceTokens(value, recursionLevel + 1)
        return definition

    _replaceTokens(definition)

def convertDashboardToPdfContent(sessionKey, namespace, owner, studioDefinition, dashboardName):
    """
    Convert the given definition to pdf content. Return False if service is not enabled or available.
    """
    # ensure correct permissions and services are enabled and available
    isScheduledExportEnabled = pdf_availability.is_studio_dashboard_scheduled_export_enabled(sessionKey)
    if isScheduledExportEnabled != '1':
        logger.info('enable_studio_dashboard_scheduled_export setting in web.conf is False')
        return False
    isDceAvailable = pdf_availability.is_dce_available(sessionKey)
    if isDceAvailable is False:
        logger.info('Dashboard Content Export service is not available')
        return False

    # convert dashboard definition from xml to json
    xmlDefinition = et.fromstring(studioDefinition)
    theme = xmlDefinition.attrib['theme'] if 'theme' in xmlDefinition.attrib else 'light'
    stringDefinition = xmlDefinition.find('definition').text
    definition = json.loads(stringDefinition)

    # modify definition to prepare it for conversion to pdf content
    replaceTokens(definition, sessionKey, namespace, dashboardName)
    deletedDataSources = modifyDataSourceSearches(definition, namespace, owner, sessionKey)
    visualizations = definition.get('visualizations', {})
    for vizName in visualizations:
        if hasToken(json.dumps(visualizations[vizName])):
            errorMessage = "Error for Visualization %s: Specify default value for token." % vizName
            logger.error(errorMessage)
            definition['visualizations'][vizName] = createVizError(errorMessage)
        else:
            modifyVizWithDeletedDataSources(definition, vizName, deletedDataSources)
            modifyVizDataURI(definition, vizName, sessionKey)
    modifyBackgroundImageDataURI(definition, sessionKey)

    # build data object
    data = { 
        'engineParameters': {
            'definition': definition,
            'theme': theme
        },
        'engine': STUDIO_ENGINE_TAG,
        'exportType': 'pdf'
    }

    # get SCS configuration
    scsConfiguration = pdf_availability.get_scs_configuration(sessionKey)
    if scsConfiguration is None:
        return False
    scsToken = scsConfiguration.get('scs_token')
    scsTenant = scsConfiguration.get('scs_tenant')
    scsEnvironment = scsConfiguration.get('scs_environment')
    cloudStack = scsConfiguration.get('cloud_stack')
    s3BucketName = scsConfiguration.get('s3_bucket_name')
    
    if s3BucketName is not None and cloudStack is not None:
        try:
            data['engineParameters']['definition'] = uploadDefinitionToS3(definition, s3BucketName, cloudStack)
        except Exception as e:
            logger.error('error while uploading definition to s3: %s' % str(e))

    logger.info('final data object to send to DCE: %s' % str(data))

    token = 'Bearer {}'.format(scsToken)
    headers = {'Authorization': token}
    url = "https://{}/{}/dce/v1alpha1/dashboard".format(scsEnvironment, scsTenant)
    response = requests.post(
        url=url,
        headers=headers,
        json=data
    )
    logger.debug('dashboard response = {}'.format(response.json()))
    operationLocation = response.json()['operationLocation']

    # poll status URL until succeeded
    response = requests.get(
        url=operationLocation,
        headers=headers
    )
    logger.debug('status response = {}'.format(response.json()))
    status = response.json()['status']
    while status == 'running' or status == 'notstarted':
        time.sleep(15)
        response = requests.get(
            url=operationLocation,
            headers=headers
        )
        logger.debug('status response = {}'.format(response.json()))
        status = response.json()['status']
    logger.debug(status)
    resourceLocation = response.json()['resourceLocation']

    # fetch pdf content from resourceLocation URL
    response = requests.get(
        url=resourceLocation,
        headers=headers
    )
    logger.debug('received encoded pdf content string')
    pdfContent = response.content
    return pdfContent